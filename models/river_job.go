// Code generated by SQLBoiler 4.18.0 (https://github.com/volatiletech/sqlboiler). DO NOT EDIT.
// This file is meant to be re-generated in place and/or deleted at any time.

package models

import (
	"context"
	"database/sql"
	"fmt"
	"reflect"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/friendsofgo/errors"
	"github.com/volatiletech/null/v8"
	"github.com/volatiletech/sqlboiler/v4/boil"
	"github.com/volatiletech/sqlboiler/v4/queries"
	"github.com/volatiletech/sqlboiler/v4/queries/qm"
	"github.com/volatiletech/sqlboiler/v4/queries/qmhelper"
	"github.com/volatiletech/sqlboiler/v4/types"
	"github.com/volatiletech/strmangle"
)

// RiverJob is an object representing the database table.
type RiverJob struct {
	ID           int64             `boil:"id" json:"id" toml:"id" yaml:"id"`
	State        string            `boil:"state" json:"state" toml:"state" yaml:"state"`
	Attempt      int16             `boil:"attempt" json:"attempt" toml:"attempt" yaml:"attempt"`
	MaxAttempts  int16             `boil:"max_attempts" json:"max_attempts" toml:"max_attempts" yaml:"max_attempts"`
	AttemptedAt  null.Time         `boil:"attempted_at" json:"attempted_at,omitempty" toml:"attempted_at" yaml:"attempted_at,omitempty"`
	CreatedAt    time.Time         `boil:"created_at" json:"created_at" toml:"created_at" yaml:"created_at"`
	FinalizedAt  null.Time         `boil:"finalized_at" json:"finalized_at,omitempty" toml:"finalized_at" yaml:"finalized_at,omitempty"`
	ScheduledAt  time.Time         `boil:"scheduled_at" json:"scheduled_at" toml:"scheduled_at" yaml:"scheduled_at"`
	Priority     int16             `boil:"priority" json:"priority" toml:"priority" yaml:"priority"`
	Args         types.JSON        `boil:"args" json:"args" toml:"args" yaml:"args"`
	AttemptedBy  types.StringArray `boil:"attempted_by" json:"attempted_by,omitempty" toml:"attempted_by" yaml:"attempted_by,omitempty"`
	Errors       types.StringArray `boil:"errors" json:"errors,omitempty" toml:"errors" yaml:"errors,omitempty"`
	Kind         string            `boil:"kind" json:"kind" toml:"kind" yaml:"kind"`
	Metadata     types.JSON        `boil:"metadata" json:"metadata" toml:"metadata" yaml:"metadata"`
	Queue        string            `boil:"queue" json:"queue" toml:"queue" yaml:"queue"`
	Tags         types.StringArray `boil:"tags" json:"tags" toml:"tags" yaml:"tags"`
	UniqueKey    null.Bytes        `boil:"unique_key" json:"unique_key,omitempty" toml:"unique_key" yaml:"unique_key,omitempty"`
	UniqueStates null.String       `boil:"unique_states" json:"unique_states,omitempty" toml:"unique_states" yaml:"unique_states,omitempty"`

	R *riverJobR `boil:"-" json:"-" toml:"-" yaml:"-"`
	L riverJobL  `boil:"-" json:"-" toml:"-" yaml:"-"`
}

var RiverJobColumns = struct {
	ID           string
	State        string
	Attempt      string
	MaxAttempts  string
	AttemptedAt  string
	CreatedAt    string
	FinalizedAt  string
	ScheduledAt  string
	Priority     string
	Args         string
	AttemptedBy  string
	Errors       string
	Kind         string
	Metadata     string
	Queue        string
	Tags         string
	UniqueKey    string
	UniqueStates string
}{
	ID:           "id",
	State:        "state",
	Attempt:      "attempt",
	MaxAttempts:  "max_attempts",
	AttemptedAt:  "attempted_at",
	CreatedAt:    "created_at",
	FinalizedAt:  "finalized_at",
	ScheduledAt:  "scheduled_at",
	Priority:     "priority",
	Args:         "args",
	AttemptedBy:  "attempted_by",
	Errors:       "errors",
	Kind:         "kind",
	Metadata:     "metadata",
	Queue:        "queue",
	Tags:         "tags",
	UniqueKey:    "unique_key",
	UniqueStates: "unique_states",
}

var RiverJobTableColumns = struct {
	ID           string
	State        string
	Attempt      string
	MaxAttempts  string
	AttemptedAt  string
	CreatedAt    string
	FinalizedAt  string
	ScheduledAt  string
	Priority     string
	Args         string
	AttemptedBy  string
	Errors       string
	Kind         string
	Metadata     string
	Queue        string
	Tags         string
	UniqueKey    string
	UniqueStates string
}{
	ID:           "river_job.id",
	State:        "river_job.state",
	Attempt:      "river_job.attempt",
	MaxAttempts:  "river_job.max_attempts",
	AttemptedAt:  "river_job.attempted_at",
	CreatedAt:    "river_job.created_at",
	FinalizedAt:  "river_job.finalized_at",
	ScheduledAt:  "river_job.scheduled_at",
	Priority:     "river_job.priority",
	Args:         "river_job.args",
	AttemptedBy:  "river_job.attempted_by",
	Errors:       "river_job.errors",
	Kind:         "river_job.kind",
	Metadata:     "river_job.metadata",
	Queue:        "river_job.queue",
	Tags:         "river_job.tags",
	UniqueKey:    "river_job.unique_key",
	UniqueStates: "river_job.unique_states",
}

// Generated where

type whereHelperint16 struct{ field string }

func (w whereHelperint16) EQ(x int16) qm.QueryMod  { return qmhelper.Where(w.field, qmhelper.EQ, x) }
func (w whereHelperint16) NEQ(x int16) qm.QueryMod { return qmhelper.Where(w.field, qmhelper.NEQ, x) }
func (w whereHelperint16) LT(x int16) qm.QueryMod  { return qmhelper.Where(w.field, qmhelper.LT, x) }
func (w whereHelperint16) LTE(x int16) qm.QueryMod { return qmhelper.Where(w.field, qmhelper.LTE, x) }
func (w whereHelperint16) GT(x int16) qm.QueryMod  { return qmhelper.Where(w.field, qmhelper.GT, x) }
func (w whereHelperint16) GTE(x int16) qm.QueryMod { return qmhelper.Where(w.field, qmhelper.GTE, x) }
func (w whereHelperint16) IN(slice []int16) qm.QueryMod {
	values := make([]interface{}, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereIn(fmt.Sprintf("%s IN ?", w.field), values...)
}
func (w whereHelperint16) NIN(slice []int16) qm.QueryMod {
	values := make([]interface{}, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereNotIn(fmt.Sprintf("%s NOT IN ?", w.field), values...)
}

type whereHelpertypes_StringArray struct{ field string }

func (w whereHelpertypes_StringArray) EQ(x types.StringArray) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, false, x)
}
func (w whereHelpertypes_StringArray) NEQ(x types.StringArray) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, true, x)
}
func (w whereHelpertypes_StringArray) LT(x types.StringArray) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LT, x)
}
func (w whereHelpertypes_StringArray) LTE(x types.StringArray) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LTE, x)
}
func (w whereHelpertypes_StringArray) GT(x types.StringArray) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GT, x)
}
func (w whereHelpertypes_StringArray) GTE(x types.StringArray) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GTE, x)
}

func (w whereHelpertypes_StringArray) IsNull() qm.QueryMod { return qmhelper.WhereIsNull(w.field) }
func (w whereHelpertypes_StringArray) IsNotNull() qm.QueryMod {
	return qmhelper.WhereIsNotNull(w.field)
}

type whereHelpernull_Bytes struct{ field string }

func (w whereHelpernull_Bytes) EQ(x null.Bytes) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, false, x)
}
func (w whereHelpernull_Bytes) NEQ(x null.Bytes) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, true, x)
}
func (w whereHelpernull_Bytes) LT(x null.Bytes) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LT, x)
}
func (w whereHelpernull_Bytes) LTE(x null.Bytes) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LTE, x)
}
func (w whereHelpernull_Bytes) GT(x null.Bytes) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GT, x)
}
func (w whereHelpernull_Bytes) GTE(x null.Bytes) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GTE, x)
}

func (w whereHelpernull_Bytes) IsNull() qm.QueryMod    { return qmhelper.WhereIsNull(w.field) }
func (w whereHelpernull_Bytes) IsNotNull() qm.QueryMod { return qmhelper.WhereIsNotNull(w.field) }

type whereHelpernull_String struct{ field string }

func (w whereHelpernull_String) EQ(x null.String) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, false, x)
}
func (w whereHelpernull_String) NEQ(x null.String) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, true, x)
}
func (w whereHelpernull_String) LT(x null.String) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LT, x)
}
func (w whereHelpernull_String) LTE(x null.String) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LTE, x)
}
func (w whereHelpernull_String) GT(x null.String) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GT, x)
}
func (w whereHelpernull_String) GTE(x null.String) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GTE, x)
}
func (w whereHelpernull_String) LIKE(x null.String) qm.QueryMod {
	return qm.Where(w.field+" LIKE ?", x)
}
func (w whereHelpernull_String) NLIKE(x null.String) qm.QueryMod {
	return qm.Where(w.field+" NOT LIKE ?", x)
}
func (w whereHelpernull_String) ILIKE(x null.String) qm.QueryMod {
	return qm.Where(w.field+" ILIKE ?", x)
}
func (w whereHelpernull_String) NILIKE(x null.String) qm.QueryMod {
	return qm.Where(w.field+" NOT ILIKE ?", x)
}
func (w whereHelpernull_String) SIMILAR(x null.String) qm.QueryMod {
	return qm.Where(w.field+" SIMILAR TO ?", x)
}
func (w whereHelpernull_String) NSIMILAR(x null.String) qm.QueryMod {
	return qm.Where(w.field+" NOT SIMILAR TO ?", x)
}
func (w whereHelpernull_String) IN(slice []string) qm.QueryMod {
	values := make([]interface{}, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereIn(fmt.Sprintf("%s IN ?", w.field), values...)
}
func (w whereHelpernull_String) NIN(slice []string) qm.QueryMod {
	values := make([]interface{}, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereNotIn(fmt.Sprintf("%s NOT IN ?", w.field), values...)
}

func (w whereHelpernull_String) IsNull() qm.QueryMod    { return qmhelper.WhereIsNull(w.field) }
func (w whereHelpernull_String) IsNotNull() qm.QueryMod { return qmhelper.WhereIsNotNull(w.field) }

var RiverJobWhere = struct {
	ID           whereHelperint64
	State        whereHelperstring
	Attempt      whereHelperint16
	MaxAttempts  whereHelperint16
	AttemptedAt  whereHelpernull_Time
	CreatedAt    whereHelpertime_Time
	FinalizedAt  whereHelpernull_Time
	ScheduledAt  whereHelpertime_Time
	Priority     whereHelperint16
	Args         whereHelpertypes_JSON
	AttemptedBy  whereHelpertypes_StringArray
	Errors       whereHelpertypes_StringArray
	Kind         whereHelperstring
	Metadata     whereHelpertypes_JSON
	Queue        whereHelperstring
	Tags         whereHelpertypes_StringArray
	UniqueKey    whereHelpernull_Bytes
	UniqueStates whereHelpernull_String
}{
	ID:           whereHelperint64{field: "\"tesla_oracle\".\"river_job\".\"id\""},
	State:        whereHelperstring{field: "\"tesla_oracle\".\"river_job\".\"state\""},
	Attempt:      whereHelperint16{field: "\"tesla_oracle\".\"river_job\".\"attempt\""},
	MaxAttempts:  whereHelperint16{field: "\"tesla_oracle\".\"river_job\".\"max_attempts\""},
	AttemptedAt:  whereHelpernull_Time{field: "\"tesla_oracle\".\"river_job\".\"attempted_at\""},
	CreatedAt:    whereHelpertime_Time{field: "\"tesla_oracle\".\"river_job\".\"created_at\""},
	FinalizedAt:  whereHelpernull_Time{field: "\"tesla_oracle\".\"river_job\".\"finalized_at\""},
	ScheduledAt:  whereHelpertime_Time{field: "\"tesla_oracle\".\"river_job\".\"scheduled_at\""},
	Priority:     whereHelperint16{field: "\"tesla_oracle\".\"river_job\".\"priority\""},
	Args:         whereHelpertypes_JSON{field: "\"tesla_oracle\".\"river_job\".\"args\""},
	AttemptedBy:  whereHelpertypes_StringArray{field: "\"tesla_oracle\".\"river_job\".\"attempted_by\""},
	Errors:       whereHelpertypes_StringArray{field: "\"tesla_oracle\".\"river_job\".\"errors\""},
	Kind:         whereHelperstring{field: "\"tesla_oracle\".\"river_job\".\"kind\""},
	Metadata:     whereHelpertypes_JSON{field: "\"tesla_oracle\".\"river_job\".\"metadata\""},
	Queue:        whereHelperstring{field: "\"tesla_oracle\".\"river_job\".\"queue\""},
	Tags:         whereHelpertypes_StringArray{field: "\"tesla_oracle\".\"river_job\".\"tags\""},
	UniqueKey:    whereHelpernull_Bytes{field: "\"tesla_oracle\".\"river_job\".\"unique_key\""},
	UniqueStates: whereHelpernull_String{field: "\"tesla_oracle\".\"river_job\".\"unique_states\""},
}

// RiverJobRels is where relationship names are stored.
var RiverJobRels = struct {
}{}

// riverJobR is where relationships are stored.
type riverJobR struct {
}

// NewStruct creates a new relationship struct
func (*riverJobR) NewStruct() *riverJobR {
	return &riverJobR{}
}

// riverJobL is where Load methods for each relationship are stored.
type riverJobL struct{}

var (
	riverJobAllColumns            = []string{"id", "state", "attempt", "max_attempts", "attempted_at", "created_at", "finalized_at", "scheduled_at", "priority", "args", "attempted_by", "errors", "kind", "metadata", "queue", "tags", "unique_key", "unique_states"}
	riverJobColumnsWithoutDefault = []string{"max_attempts", "args", "kind"}
	riverJobColumnsWithDefault    = []string{"id", "state", "attempt", "attempted_at", "created_at", "finalized_at", "scheduled_at", "priority", "attempted_by", "errors", "metadata", "queue", "tags", "unique_key", "unique_states"}
	riverJobPrimaryKeyColumns     = []string{"id"}
	riverJobGeneratedColumns      = []string{}
)

type (
	// RiverJobSlice is an alias for a slice of pointers to RiverJob.
	// This should almost always be used instead of []RiverJob.
	RiverJobSlice []*RiverJob
	// RiverJobHook is the signature for custom RiverJob hook methods
	RiverJobHook func(context.Context, boil.ContextExecutor, *RiverJob) error

	riverJobQuery struct {
		*queries.Query
	}
)

// Cache for insert, update and upsert
var (
	riverJobType                 = reflect.TypeOf(&RiverJob{})
	riverJobMapping              = queries.MakeStructMapping(riverJobType)
	riverJobPrimaryKeyMapping, _ = queries.BindMapping(riverJobType, riverJobMapping, riverJobPrimaryKeyColumns)
	riverJobInsertCacheMut       sync.RWMutex
	riverJobInsertCache          = make(map[string]insertCache)
	riverJobUpdateCacheMut       sync.RWMutex
	riverJobUpdateCache          = make(map[string]updateCache)
	riverJobUpsertCacheMut       sync.RWMutex
	riverJobUpsertCache          = make(map[string]insertCache)
)

var (
	// Force time package dependency for automated UpdatedAt/CreatedAt.
	_ = time.Second
	// Force qmhelper dependency for where clause generation (which doesn't
	// always happen)
	_ = qmhelper.Where
)

var riverJobAfterSelectMu sync.Mutex
var riverJobAfterSelectHooks []RiverJobHook

var riverJobBeforeInsertMu sync.Mutex
var riverJobBeforeInsertHooks []RiverJobHook
var riverJobAfterInsertMu sync.Mutex
var riverJobAfterInsertHooks []RiverJobHook

var riverJobBeforeUpdateMu sync.Mutex
var riverJobBeforeUpdateHooks []RiverJobHook
var riverJobAfterUpdateMu sync.Mutex
var riverJobAfterUpdateHooks []RiverJobHook

var riverJobBeforeDeleteMu sync.Mutex
var riverJobBeforeDeleteHooks []RiverJobHook
var riverJobAfterDeleteMu sync.Mutex
var riverJobAfterDeleteHooks []RiverJobHook

var riverJobBeforeUpsertMu sync.Mutex
var riverJobBeforeUpsertHooks []RiverJobHook
var riverJobAfterUpsertMu sync.Mutex
var riverJobAfterUpsertHooks []RiverJobHook

// doAfterSelectHooks executes all "after Select" hooks.
func (o *RiverJob) doAfterSelectHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobAfterSelectHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeInsertHooks executes all "before insert" hooks.
func (o *RiverJob) doBeforeInsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobBeforeInsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterInsertHooks executes all "after Insert" hooks.
func (o *RiverJob) doAfterInsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobAfterInsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeUpdateHooks executes all "before Update" hooks.
func (o *RiverJob) doBeforeUpdateHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobBeforeUpdateHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterUpdateHooks executes all "after Update" hooks.
func (o *RiverJob) doAfterUpdateHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobAfterUpdateHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeDeleteHooks executes all "before Delete" hooks.
func (o *RiverJob) doBeforeDeleteHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobBeforeDeleteHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterDeleteHooks executes all "after Delete" hooks.
func (o *RiverJob) doAfterDeleteHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobAfterDeleteHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeUpsertHooks executes all "before Upsert" hooks.
func (o *RiverJob) doBeforeUpsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobBeforeUpsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterUpsertHooks executes all "after Upsert" hooks.
func (o *RiverJob) doAfterUpsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range riverJobAfterUpsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// AddRiverJobHook registers your hook function for all future operations.
func AddRiverJobHook(hookPoint boil.HookPoint, riverJobHook RiverJobHook) {
	switch hookPoint {
	case boil.AfterSelectHook:
		riverJobAfterSelectMu.Lock()
		riverJobAfterSelectHooks = append(riverJobAfterSelectHooks, riverJobHook)
		riverJobAfterSelectMu.Unlock()
	case boil.BeforeInsertHook:
		riverJobBeforeInsertMu.Lock()
		riverJobBeforeInsertHooks = append(riverJobBeforeInsertHooks, riverJobHook)
		riverJobBeforeInsertMu.Unlock()
	case boil.AfterInsertHook:
		riverJobAfterInsertMu.Lock()
		riverJobAfterInsertHooks = append(riverJobAfterInsertHooks, riverJobHook)
		riverJobAfterInsertMu.Unlock()
	case boil.BeforeUpdateHook:
		riverJobBeforeUpdateMu.Lock()
		riverJobBeforeUpdateHooks = append(riverJobBeforeUpdateHooks, riverJobHook)
		riverJobBeforeUpdateMu.Unlock()
	case boil.AfterUpdateHook:
		riverJobAfterUpdateMu.Lock()
		riverJobAfterUpdateHooks = append(riverJobAfterUpdateHooks, riverJobHook)
		riverJobAfterUpdateMu.Unlock()
	case boil.BeforeDeleteHook:
		riverJobBeforeDeleteMu.Lock()
		riverJobBeforeDeleteHooks = append(riverJobBeforeDeleteHooks, riverJobHook)
		riverJobBeforeDeleteMu.Unlock()
	case boil.AfterDeleteHook:
		riverJobAfterDeleteMu.Lock()
		riverJobAfterDeleteHooks = append(riverJobAfterDeleteHooks, riverJobHook)
		riverJobAfterDeleteMu.Unlock()
	case boil.BeforeUpsertHook:
		riverJobBeforeUpsertMu.Lock()
		riverJobBeforeUpsertHooks = append(riverJobBeforeUpsertHooks, riverJobHook)
		riverJobBeforeUpsertMu.Unlock()
	case boil.AfterUpsertHook:
		riverJobAfterUpsertMu.Lock()
		riverJobAfterUpsertHooks = append(riverJobAfterUpsertHooks, riverJobHook)
		riverJobAfterUpsertMu.Unlock()
	}
}

// One returns a single riverJob record from the query.
func (q riverJobQuery) One(ctx context.Context, exec boil.ContextExecutor) (*RiverJob, error) {
	o := &RiverJob{}

	queries.SetLimit(q.Query, 1)

	err := q.Bind(ctx, exec, o)
	if err != nil {
		if errors.Is(err, sql.ErrNoRows) {
			return nil, sql.ErrNoRows
		}
		return nil, errors.Wrap(err, "models: failed to execute a one query for river_job")
	}

	if err := o.doAfterSelectHooks(ctx, exec); err != nil {
		return o, err
	}

	return o, nil
}

// All returns all RiverJob records from the query.
func (q riverJobQuery) All(ctx context.Context, exec boil.ContextExecutor) (RiverJobSlice, error) {
	var o []*RiverJob

	err := q.Bind(ctx, exec, &o)
	if err != nil {
		return nil, errors.Wrap(err, "models: failed to assign all query results to RiverJob slice")
	}

	if len(riverJobAfterSelectHooks) != 0 {
		for _, obj := range o {
			if err := obj.doAfterSelectHooks(ctx, exec); err != nil {
				return o, err
			}
		}
	}

	return o, nil
}

// Count returns the count of all RiverJob records in the query.
func (q riverJobQuery) Count(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	var count int64

	queries.SetSelect(q.Query, nil)
	queries.SetCount(q.Query)

	err := q.Query.QueryRowContext(ctx, exec).Scan(&count)
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to count river_job rows")
	}

	return count, nil
}

// Exists checks if the row exists in the table.
func (q riverJobQuery) Exists(ctx context.Context, exec boil.ContextExecutor) (bool, error) {
	var count int64

	queries.SetSelect(q.Query, nil)
	queries.SetCount(q.Query)
	queries.SetLimit(q.Query, 1)

	err := q.Query.QueryRowContext(ctx, exec).Scan(&count)
	if err != nil {
		return false, errors.Wrap(err, "models: failed to check if river_job exists")
	}

	return count > 0, nil
}

// RiverJobs retrieves all the records using an executor.
func RiverJobs(mods ...qm.QueryMod) riverJobQuery {
	mods = append(mods, qm.From("\"tesla_oracle\".\"river_job\""))
	q := NewQuery(mods...)
	if len(queries.GetSelect(q)) == 0 {
		queries.SetSelect(q, []string{"\"tesla_oracle\".\"river_job\".*"})
	}

	return riverJobQuery{q}
}

// FindRiverJob retrieves a single record by ID with an executor.
// If selectCols is empty Find will return all columns.
func FindRiverJob(ctx context.Context, exec boil.ContextExecutor, iD int64, selectCols ...string) (*RiverJob, error) {
	riverJobObj := &RiverJob{}

	sel := "*"
	if len(selectCols) > 0 {
		sel = strings.Join(strmangle.IdentQuoteSlice(dialect.LQ, dialect.RQ, selectCols), ",")
	}
	query := fmt.Sprintf(
		"select %s from \"tesla_oracle\".\"river_job\" where \"id\"=$1", sel,
	)

	q := queries.Raw(query, iD)

	err := q.Bind(ctx, exec, riverJobObj)
	if err != nil {
		if errors.Is(err, sql.ErrNoRows) {
			return nil, sql.ErrNoRows
		}
		return nil, errors.Wrap(err, "models: unable to select from river_job")
	}

	if err = riverJobObj.doAfterSelectHooks(ctx, exec); err != nil {
		return riverJobObj, err
	}

	return riverJobObj, nil
}

// Insert a single record using an executor.
// See boil.Columns.InsertColumnSet documentation to understand column list inference for inserts.
func (o *RiverJob) Insert(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) error {
	if o == nil {
		return errors.New("models: no river_job provided for insertion")
	}

	var err error
	if !boil.TimestampsAreSkipped(ctx) {
		currTime := time.Now().In(boil.GetLocation())

		if o.CreatedAt.IsZero() {
			o.CreatedAt = currTime
		}
	}

	if err := o.doBeforeInsertHooks(ctx, exec); err != nil {
		return err
	}

	nzDefaults := queries.NonZeroDefaultSet(riverJobColumnsWithDefault, o)

	key := makeCacheKey(columns, nzDefaults)
	riverJobInsertCacheMut.RLock()
	cache, cached := riverJobInsertCache[key]
	riverJobInsertCacheMut.RUnlock()

	if !cached {
		wl, returnColumns := columns.InsertColumnSet(
			riverJobAllColumns,
			riverJobColumnsWithDefault,
			riverJobColumnsWithoutDefault,
			nzDefaults,
		)

		cache.valueMapping, err = queries.BindMapping(riverJobType, riverJobMapping, wl)
		if err != nil {
			return err
		}
		cache.retMapping, err = queries.BindMapping(riverJobType, riverJobMapping, returnColumns)
		if err != nil {
			return err
		}
		if len(wl) != 0 {
			cache.query = fmt.Sprintf("INSERT INTO \"tesla_oracle\".\"river_job\" (\"%s\") %%sVALUES (%s)%%s", strings.Join(wl, "\",\""), strmangle.Placeholders(dialect.UseIndexPlaceholders, len(wl), 1, 1))
		} else {
			cache.query = "INSERT INTO \"tesla_oracle\".\"river_job\" %sDEFAULT VALUES%s"
		}

		var queryOutput, queryReturning string

		if len(cache.retMapping) != 0 {
			queryReturning = fmt.Sprintf(" RETURNING \"%s\"", strings.Join(returnColumns, "\",\""))
		}

		cache.query = fmt.Sprintf(cache.query, queryOutput, queryReturning)
	}

	value := reflect.Indirect(reflect.ValueOf(o))
	vals := queries.ValuesFromMapping(value, cache.valueMapping)

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, vals)
	}

	if len(cache.retMapping) != 0 {
		err = exec.QueryRowContext(ctx, cache.query, vals...).Scan(queries.PtrsFromMapping(value, cache.retMapping)...)
	} else {
		_, err = exec.ExecContext(ctx, cache.query, vals...)
	}

	if err != nil {
		return errors.Wrap(err, "models: unable to insert into river_job")
	}

	if !cached {
		riverJobInsertCacheMut.Lock()
		riverJobInsertCache[key] = cache
		riverJobInsertCacheMut.Unlock()
	}

	return o.doAfterInsertHooks(ctx, exec)
}

// Update uses an executor to update the RiverJob.
// See boil.Columns.UpdateColumnSet documentation to understand column list inference for updates.
// Update does not automatically update the record in case of default values. Use .Reload() to refresh the records.
func (o *RiverJob) Update(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) (int64, error) {
	var err error
	if err = o.doBeforeUpdateHooks(ctx, exec); err != nil {
		return 0, err
	}
	key := makeCacheKey(columns, nil)
	riverJobUpdateCacheMut.RLock()
	cache, cached := riverJobUpdateCache[key]
	riverJobUpdateCacheMut.RUnlock()

	if !cached {
		wl := columns.UpdateColumnSet(
			riverJobAllColumns,
			riverJobPrimaryKeyColumns,
		)

		if !columns.IsWhitelist() {
			wl = strmangle.SetComplement(wl, []string{"created_at"})
		}
		if len(wl) == 0 {
			return 0, errors.New("models: unable to update river_job, could not build whitelist")
		}

		cache.query = fmt.Sprintf("UPDATE \"tesla_oracle\".\"river_job\" SET %s WHERE %s",
			strmangle.SetParamNames("\"", "\"", 1, wl),
			strmangle.WhereClause("\"", "\"", len(wl)+1, riverJobPrimaryKeyColumns),
		)
		cache.valueMapping, err = queries.BindMapping(riverJobType, riverJobMapping, append(wl, riverJobPrimaryKeyColumns...))
		if err != nil {
			return 0, err
		}
	}

	values := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(o)), cache.valueMapping)

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, values)
	}
	var result sql.Result
	result, err = exec.ExecContext(ctx, cache.query, values...)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to update river_job row")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to get rows affected by update for river_job")
	}

	if !cached {
		riverJobUpdateCacheMut.Lock()
		riverJobUpdateCache[key] = cache
		riverJobUpdateCacheMut.Unlock()
	}

	return rowsAff, o.doAfterUpdateHooks(ctx, exec)
}

// UpdateAll updates all rows with the specified column values.
func (q riverJobQuery) UpdateAll(ctx context.Context, exec boil.ContextExecutor, cols M) (int64, error) {
	queries.SetUpdate(q.Query, cols)

	result, err := q.Query.ExecContext(ctx, exec)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to update all for river_job")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to retrieve rows affected for river_job")
	}

	return rowsAff, nil
}

// UpdateAll updates all rows with the specified column values, using an executor.
func (o RiverJobSlice) UpdateAll(ctx context.Context, exec boil.ContextExecutor, cols M) (int64, error) {
	ln := int64(len(o))
	if ln == 0 {
		return 0, nil
	}

	if len(cols) == 0 {
		return 0, errors.New("models: update all requires at least one column argument")
	}

	colNames := make([]string, len(cols))
	args := make([]interface{}, len(cols))

	i := 0
	for name, value := range cols {
		colNames[i] = name
		args[i] = value
		i++
	}

	// Append all of the primary key values for each column
	for _, obj := range o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), riverJobPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := fmt.Sprintf("UPDATE \"tesla_oracle\".\"river_job\" SET %s WHERE %s",
		strmangle.SetParamNames("\"", "\"", 1, colNames),
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), len(colNames)+1, riverJobPrimaryKeyColumns, len(o)))

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args...)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to update all in riverJob slice")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to retrieve rows affected all in update all riverJob")
	}
	return rowsAff, nil
}

// Upsert attempts an insert using an executor, and does an update or ignore on conflict.
// See boil.Columns documentation for how to properly use updateColumns and insertColumns.
func (o *RiverJob) Upsert(ctx context.Context, exec boil.ContextExecutor, updateOnConflict bool, conflictColumns []string, updateColumns, insertColumns boil.Columns, opts ...UpsertOptionFunc) error {
	if o == nil {
		return errors.New("models: no river_job provided for upsert")
	}
	if !boil.TimestampsAreSkipped(ctx) {
		currTime := time.Now().In(boil.GetLocation())

		if o.CreatedAt.IsZero() {
			o.CreatedAt = currTime
		}
	}

	if err := o.doBeforeUpsertHooks(ctx, exec); err != nil {
		return err
	}

	nzDefaults := queries.NonZeroDefaultSet(riverJobColumnsWithDefault, o)

	// Build cache key in-line uglily - mysql vs psql problems
	buf := strmangle.GetBuffer()
	if updateOnConflict {
		buf.WriteByte('t')
	} else {
		buf.WriteByte('f')
	}
	buf.WriteByte('.')
	for _, c := range conflictColumns {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	buf.WriteString(strconv.Itoa(updateColumns.Kind))
	for _, c := range updateColumns.Cols {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	buf.WriteString(strconv.Itoa(insertColumns.Kind))
	for _, c := range insertColumns.Cols {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	for _, c := range nzDefaults {
		buf.WriteString(c)
	}
	key := buf.String()
	strmangle.PutBuffer(buf)

	riverJobUpsertCacheMut.RLock()
	cache, cached := riverJobUpsertCache[key]
	riverJobUpsertCacheMut.RUnlock()

	var err error

	if !cached {
		insert, _ := insertColumns.InsertColumnSet(
			riverJobAllColumns,
			riverJobColumnsWithDefault,
			riverJobColumnsWithoutDefault,
			nzDefaults,
		)

		update := updateColumns.UpdateColumnSet(
			riverJobAllColumns,
			riverJobPrimaryKeyColumns,
		)

		if updateOnConflict && len(update) == 0 {
			return errors.New("models: unable to upsert river_job, could not build update column list")
		}

		ret := strmangle.SetComplement(riverJobAllColumns, strmangle.SetIntersect(insert, update))

		conflict := conflictColumns
		if len(conflict) == 0 && updateOnConflict && len(update) != 0 {
			if len(riverJobPrimaryKeyColumns) == 0 {
				return errors.New("models: unable to upsert river_job, could not build conflict column list")
			}

			conflict = make([]string, len(riverJobPrimaryKeyColumns))
			copy(conflict, riverJobPrimaryKeyColumns)
		}
		cache.query = buildUpsertQueryPostgres(dialect, "\"tesla_oracle\".\"river_job\"", updateOnConflict, ret, update, conflict, insert, opts...)

		cache.valueMapping, err = queries.BindMapping(riverJobType, riverJobMapping, insert)
		if err != nil {
			return err
		}
		if len(ret) != 0 {
			cache.retMapping, err = queries.BindMapping(riverJobType, riverJobMapping, ret)
			if err != nil {
				return err
			}
		}
	}

	value := reflect.Indirect(reflect.ValueOf(o))
	vals := queries.ValuesFromMapping(value, cache.valueMapping)
	var returns []interface{}
	if len(cache.retMapping) != 0 {
		returns = queries.PtrsFromMapping(value, cache.retMapping)
	}

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, vals)
	}
	if len(cache.retMapping) != 0 {
		err = exec.QueryRowContext(ctx, cache.query, vals...).Scan(returns...)
		if errors.Is(err, sql.ErrNoRows) {
			err = nil // Postgres doesn't return anything when there's no update
		}
	} else {
		_, err = exec.ExecContext(ctx, cache.query, vals...)
	}
	if err != nil {
		return errors.Wrap(err, "models: unable to upsert river_job")
	}

	if !cached {
		riverJobUpsertCacheMut.Lock()
		riverJobUpsertCache[key] = cache
		riverJobUpsertCacheMut.Unlock()
	}

	return o.doAfterUpsertHooks(ctx, exec)
}

// Delete deletes a single RiverJob record with an executor.
// Delete will match against the primary key column to find the record to delete.
func (o *RiverJob) Delete(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if o == nil {
		return 0, errors.New("models: no RiverJob provided for delete")
	}

	if err := o.doBeforeDeleteHooks(ctx, exec); err != nil {
		return 0, err
	}

	args := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(o)), riverJobPrimaryKeyMapping)
	sql := "DELETE FROM \"tesla_oracle\".\"river_job\" WHERE \"id\"=$1"

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args...)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to delete from river_job")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to get rows affected by delete for river_job")
	}

	if err := o.doAfterDeleteHooks(ctx, exec); err != nil {
		return 0, err
	}

	return rowsAff, nil
}

// DeleteAll deletes all matching rows.
func (q riverJobQuery) DeleteAll(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if q.Query == nil {
		return 0, errors.New("models: no riverJobQuery provided for delete all")
	}

	queries.SetDelete(q.Query)

	result, err := q.Query.ExecContext(ctx, exec)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to delete all from river_job")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to get rows affected by deleteall for river_job")
	}

	return rowsAff, nil
}

// DeleteAll deletes all rows in the slice, using an executor.
func (o RiverJobSlice) DeleteAll(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if len(o) == 0 {
		return 0, nil
	}

	if len(riverJobBeforeDeleteHooks) != 0 {
		for _, obj := range o {
			if err := obj.doBeforeDeleteHooks(ctx, exec); err != nil {
				return 0, err
			}
		}
	}

	var args []interface{}
	for _, obj := range o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), riverJobPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := "DELETE FROM \"tesla_oracle\".\"river_job\" WHERE " +
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), 1, riverJobPrimaryKeyColumns, len(o))

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to delete all from riverJob slice")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to get rows affected by deleteall for river_job")
	}

	if len(riverJobAfterDeleteHooks) != 0 {
		for _, obj := range o {
			if err := obj.doAfterDeleteHooks(ctx, exec); err != nil {
				return 0, err
			}
		}
	}

	return rowsAff, nil
}

// Reload refetches the object from the database
// using the primary keys with an executor.
func (o *RiverJob) Reload(ctx context.Context, exec boil.ContextExecutor) error {
	ret, err := FindRiverJob(ctx, exec, o.ID)
	if err != nil {
		return err
	}

	*o = *ret
	return nil
}

// ReloadAll refetches every row with matching primary key column values
// and overwrites the original object slice with the newly updated slice.
func (o *RiverJobSlice) ReloadAll(ctx context.Context, exec boil.ContextExecutor) error {
	if o == nil || len(*o) == 0 {
		return nil
	}

	slice := RiverJobSlice{}
	var args []interface{}
	for _, obj := range *o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), riverJobPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := "SELECT \"tesla_oracle\".\"river_job\".* FROM \"tesla_oracle\".\"river_job\" WHERE " +
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), 1, riverJobPrimaryKeyColumns, len(*o))

	q := queries.Raw(sql, args...)

	err := q.Bind(ctx, exec, &slice)
	if err != nil {
		return errors.Wrap(err, "models: unable to reload all in RiverJobSlice")
	}

	*o = slice

	return nil
}

// RiverJobExists checks if the RiverJob row exists.
func RiverJobExists(ctx context.Context, exec boil.ContextExecutor, iD int64) (bool, error) {
	var exists bool
	sql := "select exists(select 1 from \"tesla_oracle\".\"river_job\" where \"id\"=$1 limit 1)"

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, iD)
	}
	row := exec.QueryRowContext(ctx, sql, iD)

	err := row.Scan(&exists)
	if err != nil {
		return false, errors.Wrap(err, "models: unable to check if river_job exists")
	}

	return exists, nil
}

// Exists checks if the RiverJob row exists.
func (o *RiverJob) Exists(ctx context.Context, exec boil.ContextExecutor) (bool, error) {
	return RiverJobExists(ctx, exec, o.ID)
}
